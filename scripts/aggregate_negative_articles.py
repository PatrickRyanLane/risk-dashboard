#!/usr/bin/env python3
"""
Aggregate negative article data for stock chart heatmap visualization.
üÜï CLOUD STORAGE VERSION - Reads/writes from Google Cloud Storage

UPDATED: Handles brand articles without CEO column using smart company name matching.
Includes fuzzy matching for common company name variations.

Output: gs://BUCKET_NAME/data/daily_counts/negative-articles-summary.csv
"""

import argparse
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta, timezone
import re
import os
import sys

from storage_utils import CloudStorageManager

COMPANY_ALIASES = {
    "altice usa": "Optimum",
    "nationwide insurance": "Nationwide Mutual Insurance Company",
    "pge": "Pacific Gas and Electric Company",
    "pg&e": "Pacific Gas and Electric Company",
    "pge corporation": "Pacific Gas and Electric Company",
    "pge corp": "Pacific Gas and Electric Company",
    "pacific gas & electric": "Pacific Gas and Electric Company",
    "pacific gas and electric": "Pacific Gas and Electric Company",
    "peter kiewit sons'": "Kiewit Corporation",
    "peter kiewit sons": "Kiewit Corporation",
    "comerica": "Fifth Third Bancorp",
}

SERP_GATE_WEIGHT = 2
PERCENTILE_CUTOFF = 0.97
MIN_HISTORY_POINTS = 14
HARD_FLOOR_NEW_CO = 15


def normalize_company_name(name):
    """
    Normalize company names for matching.
    Handles common variations like "Apple Inc." vs "Apple"
    """
    if not name or name == 'nan':
        return ''
    
    name = str(name).strip()
    alias = COMPANY_ALIASES.get(name.lower())
    if alias:
        name = alias
    
    # Remove common suffixes
    suffixes = [
        ' Inc.', ' Inc', ' Corporation', ' Corp.', ' Corp',
        ' Company', ' Co.', ' Co', ' LLC', ' L.L.C.', ' LP', ' L.P.',
        ' Ltd.', ' Ltd', ' Limited', ' PLC', ' plc',
        '.com', '.net', '.org'
    ]
    
    for suffix in suffixes:
        if name.endswith(suffix):
            name = name[:-len(suffix)].strip()
    
    # Remove special characters but keep spaces
    name = re.sub(r'[^\w\s]', '', name)
    
    # Normalize whitespace
    name = ' '.join(name.split())
    
    return name


def parse_bool(value):
    return str(value).strip().lower() in {"true", "1", "yes", "y", "controlled"}


def load_serp_counts_for_date(storage, date_str):
    brand_counts = {}
    ceo_counts = {}

    brand_path = f"data/processed_serps/{date_str}-brand-serps-modal.csv"
    ceo_path = f"data/processed_serps/{date_str}-ceo-serps-modal.csv"

    if storage.file_exists(brand_path):
        df = storage.read_csv(brand_path)
        if not df.empty:
            df.columns = [c.lower().strip() for c in df.columns]
            if "company" in df.columns and "sentiment" in df.columns and "controlled" in df.columns:
                df["sentiment"] = df["sentiment"].astype(str).str.lower()
                df["controlled"] = df["controlled"].apply(parse_bool)
                mask = (df["sentiment"] == "negative") & (~df["controlled"])
                brand_counts = df[mask].groupby("company").size().to_dict()

    if storage.file_exists(ceo_path):
        df = storage.read_csv(ceo_path)
        if not df.empty:
            df.columns = [c.lower().strip() for c in df.columns]
            if "company" in df.columns and "ceo" in df.columns and "sentiment" in df.columns and "controlled" in df.columns:
                df["sentiment"] = df["sentiment"].astype(str).str.lower()
                df["controlled"] = df["controlled"].apply(parse_bool)
                mask = (df["sentiment"] == "negative") & (~df["controlled"])
                ceo_counts = df[mask].groupby(["company", "ceo"]).size().to_dict()

    return brand_counts, ceo_counts


def load_roster(storage, roster_path='rosters/main-roster.csv'):
    """
    üÜï Load roster from Cloud Storage and create company -> CEO mapping with fuzzy matching support.
    
    Args:
        storage: CloudStorageManager instance
        roster_path: Path in bucket to roster CSV
    
    Returns:
        tuple: (exact_mapping, normalized_mapping, all_companies)
    """
    try:
        # üÜï Read from Cloud Storage instead of local file
        df = storage.read_csv(roster_path)
        
        # Normalize column names
        df.columns = [c.strip().lower() for c in df.columns]
        
        # Extract company and CEO columns
        if 'company' not in df.columns or 'ceo' not in df.columns:
            print(f"‚ö†Ô∏è  Roster missing 'company' or 'ceo' columns")
            return {}, {}, set()
        
        # Clean up
        df['company'] = df['company'].astype(str).str.strip()
        df['ceo'] = df['ceo'].astype(str).str.strip()
        
        # Filter out invalid rows
        df = df[(df['company'] != '') & (df['company'] != 'nan') & 
                (df['ceo'] != '') & (df['ceo'] != 'nan')]
        
        # Create exact mapping
        exact_mapping = dict(zip(df['company'], df['ceo']))
        
        # Create normalized mapping for fuzzy matching
        normalized_mapping = {}
        for company, ceo in exact_mapping.items():
            normalized = normalize_company_name(company)
            if normalized:
                normalized_mapping[normalized.lower()] = ceo
        
        all_companies = set(df['company'].tolist())
        
        print(f"üìã Loaded roster: {len(exact_mapping)} company-CEO mappings")
        
        return exact_mapping, normalized_mapping, all_companies
        
    except Exception as e:
        print(f"‚ùå Error loading roster: {e}")
        return {}, {}, set()


def find_ceo_for_company(company, exact_mapping, normalized_mapping):
    """
    Find CEO for a company using exact then fuzzy matching.
    (No changes needed - this is a pure logic function)
    """
    if not company or company == 'nan':
        return None, None
    
    # Try exact match first
    if company in exact_mapping:
        return exact_mapping[company], 'exact'
    
    # Try case-insensitive exact match
    for roster_company, ceo in exact_mapping.items():
        if roster_company.lower() == company.lower():
            return ceo, 'case-insensitive'
    
    # Try normalized match
    normalized = normalize_company_name(company).lower()
    if normalized in normalized_mapping:
        return normalized_mapping[normalized], 'normalized'
    
    # Try partial match
    company_lower = company.lower()
    for roster_company, ceo in exact_mapping.items():
        roster_lower = roster_company.lower()
        if company_lower in roster_lower or roster_lower in company_lower:
            if len(company_lower) >= 3 and len(roster_lower) >= 3:
                return ceo, 'partial'
    
    return None, None


def process_ceo_articles(storage, file_path):
    """
    üÜï Process CEO articles file from Cloud Storage (has CEO column).
    
    Args:
        storage: CloudStorageManager instance
        file_path: Path in bucket to CSV file
    """
    # üÜï Check if file exists in Cloud Storage
    if not storage.file_exists(file_path):
        return []
    
    try:
        # üÜï Read from Cloud Storage instead of local file
        df = storage.read_csv(file_path)
        
        if df.empty:
            return []
        
        # Normalize column names
        df.columns = [c.lower().strip() for c in df.columns]
        
        # Ensure required columns exist
        required_cols = ['ceo', 'company', 'sentiment', 'title']
        for col in required_cols:
            if col not in df.columns:
                return []
        
        # Clean up data
        df['sentiment'] = df['sentiment'].astype(str).str.lower().str.strip()
        df['ceo'] = df['ceo'].astype(str).str.strip()
        df['company'] = df['company'].astype(str).str.strip()
        df['title'] = df['title'].astype(str).str.strip()
        
        # Filter for negative sentiment only
        negative = df[df['sentiment'] == 'negative']
        
        if negative.empty:
            return []
        
        summary_data = []
        
        # Group by company/CEO and aggregate
        for (ceo, company), group in negative.groupby(['ceo', 'company']):
            if not ceo or not company or ceo == 'nan' or company == 'nan':
                continue
                
            count = len(group)
            
            # Get top 3 headlines
            headlines = []
            for title in group['title'].head(3):
                title_str = str(title).strip()
                if len(title_str) > 80:
                    title_str = title_str[:77] + '...'
                headlines.append(title_str)
            
            summary_data.append({
                'ceo': ceo,
                'company': company,
                'negative_count': count,
                'top_headlines': '|'.join(headlines),
                'article_type': 'ceo'
            })
        
        return summary_data
    
    except Exception as e:
        return []


def process_brand_articles(storage, file_path, exact_mapping, normalized_mapping):
    """
    üÜï Process brand articles file from Cloud Storage (NO CEO column - look it up with fuzzy matching).
    
    Args:
        storage: CloudStorageManager instance
        file_path: Path in bucket to CSV file
        exact_mapping: Dict of company -> CEO
        normalized_mapping: Dict of normalized company -> CEO
    """
    # üÜï Check if file exists in Cloud Storage
    if not storage.file_exists(file_path):
        return []
    
    try:
        # üÜï Read from Cloud Storage instead of local file
        df = storage.read_csv(file_path)
        
        if df.empty:
            return []
        
        # Rest of the logic remains the same...
        df.columns = [c.lower().strip() for c in df.columns]
        
        required_cols = ['company', 'sentiment', 'title']
        for col in required_cols:
            if col not in df.columns:
                return []
        
        df['sentiment'] = df['sentiment'].astype(str).str.lower().str.strip()
        df['company'] = df['company'].astype(str).str.strip()
        df['title'] = df['title'].astype(str).str.strip()
        
        negative = df[df['sentiment'] == 'negative']
        
        if negative.empty:
            return []
        
        summary_data = []
        unmatched_companies = set()
        match_stats = {'exact': 0, 'case-insensitive': 0, 'normalized': 0, 'partial': 0, 'failed': 0}
        
        for company, group in negative.groupby('company'):
            if not company or company == 'nan':
                continue
            
            ceo, match_type = find_ceo_for_company(company, exact_mapping, normalized_mapping)
            
            if not ceo:
                unmatched_companies.add(company)
                match_stats['failed'] += 1
                continue
            
            match_stats[match_type] += 1
                
            count = len(group)
            
            headlines = []
            for title in group['title'].head(3):
                title_str = str(title).strip()
                if len(title_str) > 80:
                    title_str = title_str[:77] + '...'
                headlines.append(title_str)
            
            summary_data.append({
                'ceo': ceo,
                'company': company,
                'negative_count': count,
                'top_headlines': '|'.join(headlines),
                'article_type': 'brand'
            })
        
        if match_stats['failed'] > 0:
            print(f"  ‚ö†Ô∏è  {match_stats['failed']} companies not matched to CEOs")
            if unmatched_companies and len(unmatched_companies) <= 10:
                print(f"     Unmatched: {', '.join(sorted(unmatched_companies))}")
        
        matched_total = sum(v for k, v in match_stats.items() if k != 'failed')
        if matched_total > 0:
            print(f"  ‚úì {matched_total} companies matched successfully")
            if match_stats['normalized'] > 0 or match_stats['partial'] > 0:
                print(f"     (including {match_stats['normalized']} normalized, {match_stats['partial']} partial)")
        
        return summary_data
    
    except Exception as e:
        print(f"‚ö†Ô∏è  Error processing {file_path}: {e}")
        return []


def create_negative_summary(days_back=90, roster_path='rosters/main-roster.csv', bucket_name='risk-dashboard', use_local=False):
    """
    üÜï Create aggregated negative articles summary from last N days using Cloud Storage.
    
    Args:
        days_back: Number of days to scan
        roster_path: Path to roster in bucket
        bucket_name: GCS bucket name (default: risk-dashboard)
        use_local: If True, use local file storage instead of GCS
    """
    # Initialize storage (GCS by default, local with use_local=True)
    if use_local:
        print("üìÅ Using local file storage (--local flag)")
        print("‚ö†Ô∏è  Local mode not fully implemented for aggregate_negative_articles.py")
        print("   This script is designed for Cloud Storage. Use --bucket flag.")
        return
    
    storage = CloudStorageManager(bucket_name)
    print(f"‚òÅÔ∏è  Connected to bucket: {storage.bucket_name}")
    
    # üÜï Define paths in Cloud Storage (not local filesystem)
    articles_prefix = "data/processed_articles/"
    output_file = "data/daily_counts/negative-articles-summary.csv"
    
    # Load roster for company -> CEO mapping
    exact_mapping, normalized_mapping, all_companies = load_roster(storage, roster_path)
    if not exact_mapping:
        print("‚ö†Ô∏è  Warning: No roster loaded. Brand articles will be skipped.")
    
    all_summary_data = []
    today = datetime.now(timezone.utc)
    
    print(f"\nüîç Scanning last {days_back} days for negative articles...")
    
    days_processed = 0
    ceo_files_found = 0
    brand_files_found = 0
    ceo_articles_count = 0
    brand_articles_count = 0
    serp_brand_by_date = {}
    serp_ceo_by_date = {}
    
    for i in range(days_back):
        date = (today - timedelta(days=i)).strftime("%Y-%m-%d")
        
        # üÜï Build Cloud Storage paths
        ceo_file = f"{articles_prefix}{date}-ceo-articles-modal.csv"
        brand_file = f"{articles_prefix}{date}-brand-articles-modal.csv"
        
        # Load SERP counts for the day
        serp_brand, serp_ceo = load_serp_counts_for_date(storage, date)
        if serp_brand:
            serp_brand_by_date[date] = serp_brand
        if serp_ceo:
            serp_ceo_by_date[date] = serp_ceo

        # Process CEO articles
        if storage.file_exists(ceo_file):
            ceo_files_found += 1
            ceo_data = process_ceo_articles(storage, ceo_file)
            for item in ceo_data:
                item['date'] = date
                all_summary_data.append(item)
                ceo_articles_count += 1
        
        # Process brand articles
        if storage.file_exists(brand_file):
            brand_files_found += 1
            brand_data = process_brand_articles(storage, brand_file, exact_mapping, normalized_mapping)
            for item in brand_data:
                item['date'] = date
                all_summary_data.append(item)
                brand_articles_count += 1
        
        if storage.file_exists(ceo_file) or storage.file_exists(brand_file):
            days_processed += 1
    
    print(f"\nüìÅ Files found: {ceo_files_found} CEO, {brand_files_found} brand ({days_processed} days with data)")
    print(f"üìä Article summaries created: {ceo_articles_count} CEO, {brand_articles_count} brand")
    
    # Create summary DataFrame
    if all_summary_data:
        summary_df = pd.DataFrame(all_summary_data)
        summary_df = summary_df.sort_values(['company', 'date', 'article_type'])
        summary_df = summary_df[[
            'date', 'company', 'ceo', 'negative_count', 'top_headlines', 'article_type',
            'serp_neg_uncontrolled', 'baseline_p97', 'risk_score'
        ]]

        def lookup_serp_count(row):
            if row['article_type'] == 'ceo':
                return serp_ceo_by_date.get(row['date'], {}).get((row['company'], row['ceo']), 0)
            return serp_brand_by_date.get(row['date'], {}).get(row['company'], 0)

        summary_df['serp_neg_uncontrolled'] = summary_df.apply(lookup_serp_count, axis=1)

        brand_stats = summary_df[summary_df['article_type'] == 'brand'].groupby('company')['negative_count'] \
            .agg(['count', lambda x: x.quantile(PERCENTILE_CUTOFF)]).to_dict('index')
        ceo_stats = summary_df[summary_df['article_type'] == 'ceo'].groupby('company')['negative_count'] \
            .agg(['count', lambda x: x.quantile(PERCENTILE_CUTOFF)]).to_dict('index')

        def baseline_for_row(row):
            stats_lookup = ceo_stats if row['article_type'] == 'ceo' else brand_stats
            company_stats = stats_lookup.get(row['company'], {'count': 0, '<lambda_0>': 0})
            if company_stats['count'] >= MIN_HISTORY_POINTS:
                return float(company_stats['<lambda_0>'])
            return float(HARD_FLOOR_NEW_CO)

        summary_df['baseline_p97'] = summary_df.apply(baseline_for_row, axis=1)
        summary_df['risk_score'] = (
            (summary_df['negative_count'] - summary_df['baseline_p97']).clip(lower=0) +
            (summary_df['serp_neg_uncontrolled'] * SERP_GATE_WEIGHT)
        )
        
        # Show helpful stats
        print(f"\n{'='*60}")
        print("üìä COMPANIES WITH BOTH CEO AND BRAND ARTICLES")
        print('='*60)
        
        companies_with_both = []
        for company in summary_df['company'].unique():
            company_data = summary_df[summary_df['company'] == company]
            types = company_data['article_type'].unique()
            if len(types) > 1:
                companies_with_both.append(company)
        
        if companies_with_both:
            print(f"‚úÖ {len(companies_with_both)} companies have both CEO and brand negative articles!")
            for company in sorted(companies_with_both)[:5]:
                company_data = summary_df[summary_df['company'] == company]
                ceo_count = company_data[company_data['article_type'] == 'ceo']['negative_count'].sum()
                brand_count = company_data[company_data['article_type'] == 'brand']['negative_count'].sum()
                print(f"  ‚Ä¢ {company}: {ceo_count} CEO, {brand_count} brand")
        else:
            print("‚ö†Ô∏è  No companies have both types")
            
    else:
        print("‚ö†Ô∏è  No negative articles found in the specified time range")
        summary_df = pd.DataFrame(columns=[
            'date', 'company', 'ceo', 'negative_count', 'top_headlines', 'article_type',
            'serp_neg_uncontrolled', 'baseline_p97', 'risk_score'
        ])
    
    # üÜï Write to Cloud Storage instead of local filesystem
    storage.write_csv(summary_df, output_file, index=False)
    
    print(f"\n‚úÖ Created gs://{storage.bucket_name}/{output_file}")
    print(f"üìä Total rows: {len(summary_df):,}")
    
    if not summary_df.empty:
        # üÜï Calculate approximate size (can't get exact size from bucket easily)
        csv_size_bytes = len(summary_df.to_csv(index=False).encode('utf-8'))
        file_size_kb = csv_size_bytes / 1024
        print(f"üìä File size: {file_size_kb:.1f} KB")
        
        ceo_count = len(summary_df[summary_df['article_type'] == 'ceo'])
        brand_count = len(summary_df[summary_df['article_type'] == 'brand'])
        
        print(f"üéØ CEO article summaries: {ceo_count:,}")
        print(f"üè¢ Brand article summaries: {brand_count:,}")
        print(f"üìÖ Date range: {summary_df['date'].min()} to {summary_df['date'].max()}")
        
        companies = summary_df['company'].nunique()
        print(f"üè≠ Companies with negative coverage: {companies}")

        if 'risk_score' in summary_df.columns:
            top_risk = summary_df.sort_values('risk_score', ascending=False).head(10)
            print("\nüî• Top 10 Risk Scores")
            for _, row in top_risk.iterrows():
                company = row.get('company', '')
                ceo = row.get('ceo', '')
                article_type = row.get('article_type', '')
                date = row.get('date', '')
                score = row.get('risk_score', 0)
                serp_count = row.get('serp_neg_uncontrolled', 0)
                neg_count = row.get('negative_count', 0)
                print(f"  ‚Ä¢ {date} | {company} | {article_type} | score={score:.1f} (neg={neg_count}, serp={serp_count})")
        
        # üÜï Show public URL if bucket is public
        public_url = storage.get_public_url(output_file)
        print(f"üåê Public URL: {public_url}")


def main():
    parser = argparse.ArgumentParser(
        description='Aggregate negative articles for stock chart visualization (Cloud Storage version)'
    )
    parser.add_argument(
        '--days-back',
        type=int,
        default=90,
        help='Number of days to look back (default: 90)'
    )
    parser.add_argument(
        '--roster',
        type=str,
        default='rosters/main-roster.csv',
        help='Path to roster file in bucket (default: rosters/main-roster.csv)'
    )
    parser.add_argument(
        '--bucket',
        type=str,
        default='risk-dashboard',
        help='GCS bucket name (default: risk-dashboard)'
    )
    parser.add_argument(
        '--local',
        action='store_true',
        help='Use local file storage instead of GCS'
    )
    
    args = parser.parse_args()
    
    if args.days_back < 1:
        print("‚ùå --days-back must be at least 1")
        return 1
    
    # Pass parameters to function
    create_negative_summary(
        days_back=args.days_back, 
        roster_path=args.roster,
        bucket_name=args.bucket,
        use_local=args.local
    )
    return 0


if __name__ == "__main__":
    exit(main())
